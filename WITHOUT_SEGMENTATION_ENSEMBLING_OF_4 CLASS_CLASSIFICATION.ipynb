{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "gpu=int(input(\"Which gpu number you would like to allocate:\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu)\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import keras\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from skimage.transform import radon, rescale\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from classification_models.keras import Classifiers\n",
    "from skimage import feature\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from skimage import data, exposure\n",
    "from tensorflow.keras.layers import Layer\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def no_data_augmentation(normal_files,covid_files,pneumonia_files,tb_files):\n",
    "    aug_normal=[]\n",
    "    aug_covid=[]\n",
    "    aug_pneumonia=[]\n",
    "    aug_tb=[]\n",
    "    for ele in normal_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_normal.append(pic)\n",
    "    for ele in covid_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_covid.append(pic)\n",
    "    for ele in pneumonia_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "      \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_pneumonia.append(pic)\n",
    "    \n",
    "    for ele in tb_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_tb.append(pic)    \n",
    "    for i in range(len(aug_normal)):\n",
    "        aug_normal[i]=aug_normal[i].reshape((224,224))\n",
    "    \n",
    "    for i in range(len(aug_covid)):\n",
    "        aug_covid[i]=aug_covid[i].reshape((224,224))\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        aug_pneumonia[i]=aug_pneumonia[i].reshape((224,224))\n",
    "    for i in range(len(aug_tb)):\n",
    "        aug_tb[i]=aug_tb[i].reshape((224,224))    \n",
    "    \n",
    "    print(\"Normal files without augmentation:\",len(aug_normal))\n",
    "    print(\"Covid files without augmentation:\", len(aug_covid))\n",
    "    print(\"Pneumonia files without augmentation:\",len(aug_pneumonia))\n",
    "    print(\"tb files without augmentation:\",len(aug_tb))\n",
    "    return aug_normal,aug_covid,aug_pneumonia, aug_tb\n",
    "\n",
    "def data_augmentation(normal_files,covid_files,pneumonia_files,tb_files):\n",
    "    aug_normal=[]\n",
    "    aug_covid=[]\n",
    "    thresh_hold=7\n",
    "    aug_pneumonia=[]\n",
    "    aug_tb=[]\n",
    "    \n",
    "    #x = tf.keras.preprocessing.image.load_img(\"/content/IM-0001-0001.jpeg\")\n",
    "    \n",
    "    datagen=ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "\n",
    "    )\n",
    "    #normal\n",
    "    counter=0\n",
    "    \n",
    "    for location in tqdm(normal_files):\n",
    "        counter=0\n",
    "\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "       \n",
    "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=7:\n",
    "                break\n",
    "            #i=i/255.0\n",
    "\n",
    "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "            aug_normal.append(i)\n",
    "            counter+=1\n",
    "    #tb\n",
    "    counter=0\n",
    "    \n",
    "    for location in tqdm(tb_files):\n",
    "        counter=0\n",
    "\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "       \n",
    "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=7:\n",
    "                break\n",
    "            #i=i/255.0\n",
    "\n",
    "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "            aug_tb.append(i)\n",
    "            counter+=1\n",
    "            \n",
    "    #covid\n",
    "    counter=0\n",
    "    for location in tqdm(covid_files):\n",
    "        counter=0\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "    \n",
    "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        #x=img_to_array(x)\n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=7:\n",
    "                break\n",
    "\n",
    "            aug_covid.append(i)\n",
    "            counter+=1    \n",
    "    #pneumonia\n",
    "    counter=0\n",
    "    for location in tqdm(pneumonia_files):\n",
    "        counter=0\n",
    "        x = Image.open(location).convert('L')\n",
    "        x = asarray(x)\n",
    "    \n",
    "\n",
    "        x=cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        x=x/255.0\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        #x=img_to_array(x)\n",
    "        x=x.reshape((1,)+x.shape)\n",
    "        #x=x/255.0\n",
    "\n",
    "        for i in datagen.flow(x):\n",
    "            if counter>=7:\n",
    "                break\n",
    "            #i=i/255.0\n",
    "            #i = cv2.resize(i,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "            aug_pneumonia.append(i)\n",
    "            counter+=1    \n",
    "\n",
    "    for ele in normal_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_normal.append(pic)\n",
    "    for ele in covid_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_covid.append(pic)\n",
    "    for ele in pneumonia_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "      \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_pneumonia.append(pic)\n",
    "    for ele in tb_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele).convert('L')\n",
    "        x = asarray(x)\n",
    "      \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_tb.append(pic)    \n",
    "    \n",
    "    for i in range(len(aug_normal)):\n",
    "        aug_normal[i]=aug_normal[i].reshape((224,224))\n",
    "    \n",
    "    for i in range(len(aug_covid)):\n",
    "        aug_covid[i]=aug_covid[i].reshape((224,224))\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        aug_pneumonia[i]=aug_pneumonia[i].reshape((224,224))\n",
    "    for i in range(len(aug_tb)):\n",
    "        aug_tb[i]=aug_tb[i].reshape((224,224))\n",
    "    \n",
    "    print(\"Normal files after augmentation:\",len(aug_normal))\n",
    "    print(\"Covid files after augmentation:\", len(aug_covid))\n",
    "    print(\"Pneumonia files after augmentation:\",len(aug_pneumonia))\n",
    "    print(\"TB files after augmentation:\",len(aug_tb))\n",
    "    return aug_normal,aug_covid,aug_pneumonia,aug_tb\n",
    "\n",
    "def making_full_data(aug_normal,aug_covid,aug_pneumonia,aug_tb):\n",
    "    aug_normal=shuffle(aug_normal, random_state=0)\n",
    "    aug_covid=shuffle(aug_covid,random_state=0)\n",
    "    aug_pneumonia=shuffle(aug_pneumonia,random_state=0)\n",
    "    aug_tb=shuffle(aug_tb,random_state=0)\n",
    "    \n",
    "    aug_normal_labels=[]\n",
    "    for i in range(len(aug_normal)):\n",
    "        aug_normal_labels.append(0)\n",
    "    print(np.shape(aug_normal),np.shape(aug_normal_labels))\n",
    "    aug_covid_labels=[]\n",
    "    for i in range(len(aug_covid)):\n",
    "        aug_covid_labels.append(1)\n",
    "    print(np.shape(aug_covid),np.shape(aug_covid_labels))\n",
    "    aug_pneumonia_labels=[]\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        aug_pneumonia_labels.append(2)\n",
    "    print(np.shape(aug_pneumonia),np.shape(aug_pneumonia_labels))  \n",
    "    aug_tb_labels=[]\n",
    "    for i in range(len(aug_tb)):\n",
    "        aug_tb_labels.append(3)\n",
    "    print(np.shape(aug_tb),np.shape(aug_tb_labels))  \n",
    "\n",
    "    full_data=[]\n",
    "    full_label=[]\n",
    "    for i in range(len(aug_normal)):\n",
    "        full_data.append(aug_normal[i])\n",
    "        full_label.append(aug_normal_labels[i])\n",
    "    for i in range(len(aug_covid)):\n",
    "        full_data.append(aug_covid[i])\n",
    "        full_label.append(aug_covid_labels[i])\n",
    "    for i in range(len(aug_pneumonia)):\n",
    "        full_data.append(aug_pneumonia[i])\n",
    "        full_label.append(aug_pneumonia_labels[i])\n",
    "    for i in range(len(aug_tb)):\n",
    "        full_data.append(aug_tb[i])\n",
    "        full_label.append(aug_tb_labels[i])\n",
    "        \n",
    "    full_data=np.array(full_data)\n",
    "    full_label=np.array(full_label)\n",
    "    \n",
    "    full_data=shuffle(full_data,random_state=0)\n",
    "    full_label=shuffle(full_label,random_state=0)\n",
    "    \n",
    "    return full_data,full_label\n",
    "\"\"\"Inception 2D_CNN Models in Tensorflow-Keras.\n",
    "References -\n",
    "Inception_v1 (GoogLeNet): https://arxiv.org/abs/1409.4842 [Going Deeper with Convolutions]\n",
    "Inception_v2: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
    "Inception_v3: http://arxiv.org/abs/1512.00567 [Rethinking the Inception Architecture for Computer Vision]\n",
    "Inception_v4: https://arxiv.org/abs/1602.07261 [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Conv_2D_Block(x, model_width, kernel, strides=(1, 1), padding=\"same\"):\n",
    "    # 2D Convolutional Block with BatchNormalization\n",
    "    x = tf.keras.layers.Conv2D(model_width, kernel, strides=strides, padding=padding, kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def classifier(inputs, class_number):\n",
    "    # Construct the Classifier Group\n",
    "    # inputs       : input vector\n",
    "    # class_number : number of output classes\n",
    "    out = tf.keras.layers.Dense(class_number, activation='softmax')(inputs)\n",
    "    return out\n",
    "\n",
    "\n",
    "def regressor(inputs, feature_number):\n",
    "    # Construct the Regressor Group\n",
    "    # inputs         : input vector\n",
    "    # feature_number : number of output features\n",
    "    out = tf.keras.layers.Dense(feature_number, activation='linear')(inputs)\n",
    "    return out\n",
    "\n",
    "\n",
    "def SE_Block(inputs, num_filters, ratio):\n",
    "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
    "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
    "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
    "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
    "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
    "\n",
    "    scale = inputs * excitation\n",
    "\n",
    "    return scale\n",
    "\n",
    "\n",
    "def Inceptionv1_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1), padding='valid')\n",
    "\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1), padding='valid')\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
    "\n",
    "    branch5x5 = Conv_2D_Block(inputs, filterB3_1, (1, 1), padding='valid')\n",
    "    branch5x5 = Conv_2D_Block(branch5x5, filterB3_2, (5, 5))\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inceptionv2_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inception_Module_A(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch5x5 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch5x5 = Conv_2D_Block(branch5x5, filterB2_2, (5, 5))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_A'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inception_Module_B(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch7x7 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (1, 7))\n",
    "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (7, 1))\n",
    "\n",
    "    branch7x7dbl = Conv_2D_Block(inputs, filterB3_1, 1)\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (1, 7))\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (7, 1))\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (1, 7))\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (7, 1))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=-1, name='Inception_Block_B'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inception_Module_C(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3_2 = Conv_2D_Block(branch3x3, filterB2_2, (1, 3))\n",
    "    branch3x3_3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 1))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (1, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 1))\n",
    "    branch3x3dbl_2 = Conv_2D_Block(branch3x3dbl, filterB3_3, (1, 3))\n",
    "    branch3x3dbl_3 = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 1))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch3x3_2, branch3x3_3, branch3x3dbl_2, branch3x3dbl_3, branch_pool], axis=-1, name='Inception_Block_C'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Reduction_Block_A(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
    "    # Reduction Block A (i)\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (3, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Reduction_Block_B(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
    "    # Reduction Block B (i)\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (1, 7))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (7, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class SEInception:\n",
    "    def __init__(self, length, width, num_channel, num_filters, ratio=4, problem_type='Regression',\n",
    "                 output_nums=1, pooling='avg', dropout_rate=False, auxilliary_outputs=False):\n",
    "        # length: Input Signal Length\n",
    "        # model_depth: Depth of the Model\n",
    "        # model_width: Width of the Model\n",
    "        # kernel_size: Kernel or Filter Size of the Input Convolutional Layer\n",
    "        # num_channel: Number of Channels of the Input Predictor Signals\n",
    "        # problem_type: Regression or Classification\n",
    "        # output_nums: Number of Output Classes in Classification mode and output features in Regression mode\n",
    "        # pooling: Choose either 'max' for MaxPooling or 'avg' for Averagepooling\n",
    "        # dropout_rate: If turned on, some layers will be dropped out randomly based on the selected proportion\n",
    "        # auxilliary_outputs: Two extra Auxullary outputs for the Inception models, acting like Deep Supervision\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.num_channel = num_channel\n",
    "        self.num_filters = num_filters\n",
    "        self.ratio = ratio\n",
    "        self.problem_type = problem_type\n",
    "        self.output_nums = output_nums\n",
    "        self.pooling = pooling\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.auxilliary_outputs = auxilliary_outputs\n",
    "\n",
    "    def MLP(self, x):\n",
    "        if self.pooling == 'avg':\n",
    "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        elif self.pooling == 'max':\n",
    "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "        if self.dropout_rate:\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "        # Final Dense Outputting Layer for the outputs\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        outputs = tf.keras.layers.Dense(self.output_nums, activation='linear')(x)\n",
    "        if self.problem_type == 'Classification':\n",
    "            outputs = tf.keras.layers.Dense(self.output_nums, activation='softmax')(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def SEInception_v1(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem\n",
    "        x = Conv_2D_Block(inputs, self.num_filters, 7, strides=2)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Conv_2D_Block(x, self.num_filters, 1, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 3, 3)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = Inceptionv1_Module(x, 64, 96, 128, 16, 32, 32, 1)  # Inception Block 1\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 128, 128, 192, 32, 96, 64, 2)  # Inception Block 2\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Inceptionv1_Module(x, 192, 96, 208, 16, 48, 64, 3)  # Inception Block 3\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 160, 112, 224, 24, 64, 64, 4)  # Inception Block 4\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 128, 128, 256, 24, 64, 64, 5)  # Inception Block 5\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 112, 144, 288, 32, 64, 64, 6)  # Inception Block 6\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 7)  # Inception Block 7\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 8)  # Inception Block 8\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 384, 192, 384, 48, 128, 128, 9)  # Inception Block 9\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v1')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def SEInception_v2(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem: 56 x 64\n",
    "        x = tf.keras.layers.SeparableConv2D(self.num_filters, kernel_size=7, strides=(2, 2), depth_multiplier=1, padding='same')(inputs)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Conv_2D_Block(x, self.num_filters * 2, 1, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = Inceptionv2_Module(x, 64, 64, 64, 64, 96, 96, 32, 1)  # Inception Block 1: 28 x 192\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 64, 64, 96, 64, 96, 96, 64, 2)  # Inception Block 2: 28 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 128, 160, 64, 96, 96, 1)  # Reduction Block 1: 28 x 320\n",
    "\n",
    "        x = Inceptionv2_Module(x, 224, 64, 96, 96, 128, 128, 128, 3)  # Inception Block 3: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 192, 96, 128, 96, 128, 128, 128, 4)  # Inception Block 4: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 160, 128, 160, 128, 160, 160, 96, 5)  # Inception Block 5: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 96, 128, 192, 160, 192, 192, 96, 6)  # Inception Block 6: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 128, 192, 192, 256, 256, 2)  # Reduction Block 2: 14 x 576\n",
    "\n",
    "        x = Inceptionv2_Module(x, 352, 192, 320, 160, 224, 224, 128, 7)  # Inception Block 7: 7 x 1024\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 352, 192, 320, 192, 224, 224, 128, 8)  # Inception Block 8: 7 x 1024\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v2')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def SEInception_v3(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem\n",
    "        x = Conv_2D_Block(inputs, self.num_filters, 3, strides=2, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters, 3, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 2, 3)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = Conv_2D_Block(x, self.num_filters * 2.5, 1, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        # 3x Inception-A Blocks\n",
    "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 32, 1)  # Inception-A Block 1: 35 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 2)  # Inception-A Block 2: 35 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 3)  # Inception-A Block 3: 35 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 64, 384, 64, 96, 96, 1)  # Reduction Block 1: 17 x 768\n",
    "\n",
    "        # 4x Inception-B Blocks\n",
    "        x = Inception_Module_B(x, 192, 128, 192, 128, 128, 192, 192, 1)  # Inception-B Block 1: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 2)  # Inception-B Block 2: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 3)  # Inception-B Block 3: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_B(x, 192, 192, 192, 192, 192, 192, 192, 4)  # Inception-B Block 4: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_B(x, 192, 320, 192, 192, 192, 2)  # Reduction Block 2: 8 x 1280\n",
    "\n",
    "        # 2x Inception-C Blocks: 8 x 2048\n",
    "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 1)  # Inception-C Block 1: 8 x 2048\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 2)  # Inception-C Block 2: 8 x 2048\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v3')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def SEInception_v4(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem\n",
    "        x = Conv_2D_Block(inputs, 32, 3, strides=2, padding='valid')\n",
    "        x = Conv_2D_Block(x, 32, 3, padding='valid')\n",
    "        x = Conv_2D_Block(x, 64, 3)\n",
    "\n",
    "        branch1 = Conv_2D_Block(x, 96, 3, strides=2, padding='valid')\n",
    "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
    "\n",
    "        branch1 = Conv_2D_Block(x, 64, 1)\n",
    "        branch1 = Conv_2D_Block(branch1, 96, 3, padding='valid')\n",
    "        branch2 = Conv_2D_Block(x, 64, 1)\n",
    "        branch2 = Conv_2D_Block(branch2, 64, 7)\n",
    "        branch2 = Conv_2D_Block(branch2, 96, 3, padding='valid')\n",
    "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
    "\n",
    "        branch1 = Conv_2D_Block(x, 192, 3, padding='valid')\n",
    "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x)\n",
    "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
    "\n",
    "        # 4x Inception-A Blocks - 35 x 256\n",
    "        for i in range(4):\n",
    "            x = Inception_Module_A(x, 96, 64, 96, 64, 96, 96, 96, i)\n",
    "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 96, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 64, 384, 192, 224, 256, 1)  # Reduction Block 1: 17 x 768\n",
    "\n",
    "        # 7x Inception-B Blocks - 17 x 768\n",
    "        for i in range(7):\n",
    "            x = Inception_Module_B(x, 384, 192, 256, 192, 224, 256, 128, i)\n",
    "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 128, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_B(x, 192, 192, 256, 320, 320, 2)  # Reduction Block 2: 8 x 1280\n",
    "\n",
    "        # 3x Inception-C Blocks: 8 x 2048\n",
    "        for i in range(3):\n",
    "            x = Inception_Module_C(x, 256, 384, 512, 384, 512, 512, 256, i)\n",
    "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v4')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.layers.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v4')\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def making_training_and_testing_data(full_data,full_label):\n",
    "    \n",
    "    \n",
    "    train_label=[]\n",
    "    for i in range(len(full_label)):\n",
    "        if full_label[i]==0:\n",
    "            train_label.append([0,1,0,0])\n",
    "        elif full_label[i]==1:\n",
    "            train_label.append([1,0,0,0])\n",
    "        elif full_label[i]==2:\n",
    "            train_label.append([0,0,1,0])\n",
    "        elif full_label[i]==3:\n",
    "            train_label.append([0,0,0,1])\n",
    "\n",
    "    \n",
    "    full_label=np.array(train_label)\n",
    "    \n",
    "    \n",
    "    return full_data,full_label\n",
    "    \n",
    "def my_plots(folder_path,history,my_model):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    my_path=\"training and validation accuracy curve of \"+my_model+\".png\"\n",
    "    plt.savefig(folder_path+my_path)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    #plt.ylim([-3, 3])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.25))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    my_path=\"training and validation loss curve of \"+my_model+\".png\"\n",
    "    plt.savefig(folder_path+my_path)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "def SE_Block(inputs, num_filters, ratio):\n",
    "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
    "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
    "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
    "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
    "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
    "\n",
    "    scale = inputs * excitation\n",
    "    return scale\n",
    "\n",
    "def fire_module(x, fire_id, squeeze=16, expand=64):\n",
    "    s_id = 'fire' + str(fire_id) + '/'\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = 3\n",
    "    \n",
    "    x = Conv2D(squeeze, (1, 1), padding='valid')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    left = Conv2D(expand, (1, 1), padding='valid')(x)\n",
    "    left = Activation('relu')(left)\n",
    "\n",
    "    right = Conv2D(expand, (3, 3), padding='same')(x)\n",
    "    right = Activation('relu')(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=channel_axis)\n",
    "    return x\n",
    "from keras.utils import get_file\n",
    "def SE_SQUEEZNET(inputs,ratio,num_of_class):\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
    "    x = Activation('relu', name='relu_conv1')(x)\n",
    "    x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    \n",
    "    \n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool2')(x)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    \n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    \n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    \n",
    "    \n",
    "    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool4')(x)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)    \n",
    "    \n",
    "\n",
    "    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)  \n",
    "\n",
    "    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n",
    "    #x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n",
    "    x=SE_Block(x,num_filters=int(np.shape(x)[-1]),ratio=ratio)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    output = Dense(num_of_class, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, [output])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':  #straight away go to this\n",
    "    normal_dir = \"\" #give your normal cases data path here\n",
    "    #vit_datasets/Dataset_ViT/ViT_dataset/Covid-19\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    normal_files = glob.glob(dir)\n",
    "    normal_1 = glob.glob(dir1)\n",
    "    normal_2 = glob.glob(dir2)\n",
    "    normal_files.extend(normal_1)\n",
    "    normal_files.extend(normal_2)\n",
    "\n",
    "    normal_dir = \"\"  #give your covid 19 cases data path here\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    covid_files = glob.glob(dir)\n",
    "    covid_files2 = glob.glob(dir2)\n",
    "    covid_files1 = glob.glob(dir1)\n",
    "    covid_files.extend(covid_files2)\n",
    "    covid_files.extend(covid_files1)\n",
    "\n",
    "    normal_dir = \"\" #give your pneumonia cases data path here\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    pneumonia_files = glob.glob(dir)\n",
    "    pneumonia_1 = glob.glob(dir1)\n",
    "    pneumonia_2 = glob.glob(dir2)\n",
    "    pneumonia_files.extend(pneumonia_1)\n",
    "    pneumonia_files.extend(pneumonia_2)\n",
    "\n",
    "    normal_dir = \"\" #give your TB cases data path here\n",
    "    dir1 = os.path.join(normal_dir,\"*.png\")\n",
    "    dir2 = os.path.join(normal_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(normal_dir,\"*.jpg\")\n",
    "    tb_files = glob.glob(dir)\n",
    "    tb_1 = glob.glob(dir1)\n",
    "    tb_2 = glob.glob(dir2)\n",
    "    tb_files.extend(tb_1)\n",
    "    tb_files.extend(tb_2)\n",
    "    \n",
    "    normal_files.sort()\n",
    "    covid_files.sort()\n",
    "    pneumonia_files.sort()\n",
    "    tb_files.sort()\n",
    "    normal_files=shuffle(normal_files,random_state=10)\n",
    "    covid_files=shuffle(covid_files,random_state=10)\n",
    "    pneumonia_files=shuffle(pneumonia_files,random_state=10)\n",
    "    tb_files=shuffle(tb_files,random_state=10)\n",
    "    \n",
    "    print(\"pneumonia_files:\",len(pneumonia_files))\n",
    "    print(\"covid_files:\",len(covid_files))\n",
    "    print(\"normal_files:\",len(normal_files))\n",
    "    print(\"tb_files:\",len(tb_files))\n",
    "    \n",
    "    total_files=(len(normal_files)+len(covid_files)+len(pneumonia_files)+len(tb_files))\n",
    "    \n",
    "    temp_files=[]\n",
    "    temp_labels=[]\n",
    "    for i in range(len(pneumonia_files)):\n",
    "        temp_files.append(pneumonia_files[i])\n",
    "        temp_labels.append(0)\n",
    "    \n",
    "    for i in range(len(covid_files)):\n",
    "        temp_files.append(covid_files[i])\n",
    "        temp_labels.append(1)\n",
    "    \n",
    "    for i in range(len(normal_files)):\n",
    "        temp_files.append(normal_files[i])\n",
    "        temp_labels.append(2)\n",
    "        \n",
    "    for i in range(len(tb_files)):\n",
    "        temp_files.append(tb_files[i])\n",
    "        temp_labels.append(3)\n",
    "        \n",
    "    temp_files=shuffle(temp_files,random_state=10)\n",
    "    temp_labels=shuffle(temp_labels,random_state=10)  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp_files, temp_labels, test_size=0.20, random_state=1, stratify=temp_labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=1, stratify=y_train)\n",
    "    print(len(y_train))\n",
    "    print(len(y_val))\n",
    "    print(len(y_test))\n",
    "        \n",
    "    #test\n",
    "    test_normal_files=[]\n",
    "    for i in range(len(X_test)):\n",
    "        if y_test[i]==2:\n",
    "            test_normal_files.append(X_test[i])\n",
    "    \n",
    "    test_covid_files=[]\n",
    "    for i in range(len(X_test)):\n",
    "        if y_test[i]==1:\n",
    "            test_covid_files.append(X_test[i])\n",
    "            \n",
    "    test_pneumonia_files=[]\n",
    "    for i in range(len(X_test)):\n",
    "        if y_test[i]==0:\n",
    "            test_pneumonia_files.append(X_test[i])\n",
    "            \n",
    "    test_tb_files=[]\n",
    "    for i in range(len(X_test)):\n",
    "        if y_test[i]==3:\n",
    "            test_tb_files.append(X_test[i])\n",
    "    \n",
    "    #train\n",
    "    train_normal_files=[]\n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i]==2:\n",
    "            train_normal_files.append(X_train[i])\n",
    "    \n",
    "    train_covid_files=[]\n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i]==1:\n",
    "            train_covid_files.append(X_train[i])\n",
    "            \n",
    "    train_pneumonia_files=[]\n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i]==0:\n",
    "            train_pneumonia_files.append(X_train[i])\n",
    "            \n",
    "    train_tb_files=[]\n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i]==3:\n",
    "            train_tb_files.append(X_train[i])\n",
    "            \n",
    "    #val\n",
    "    val_normal_files=[]\n",
    "    for i in range(len(X_val)):\n",
    "        if y_val[i]==2:\n",
    "            val_normal_files.append(X_val[i])\n",
    "    \n",
    "    val_covid_files=[]\n",
    "    for i in range(len(X_val)):\n",
    "        if y_val[i]==1:\n",
    "            val_covid_files.append(X_val[i])\n",
    "            \n",
    "    val_pneumonia_files=[]\n",
    "    for i in range(len(X_val)):\n",
    "        if y_val[i]==0:\n",
    "            val_pneumonia_files.append(X_val[i])\n",
    "            \n",
    "    val_tb_files=[]\n",
    "    for i in range(len(X_val)):\n",
    "        if y_val[i]==3:\n",
    "            val_tb_files.append(X_val[i])\n",
    "    \n",
    "    \n",
    "    print(\"test normal:\",len(test_normal_files))\n",
    "    print(\"test covid:\",len(test_covid_files))\n",
    "    print(\"test pneumonia:\",len(test_pneumonia_files))\n",
    "    print(\"test tb:\",len(test_tb_files))\n",
    "    print(\"val normal:\",len(val_normal_files))\n",
    "    print(\"val covid:\",len(val_covid_files))\n",
    "    print(\"val pneumonia:\",len(val_pneumonia_files))\n",
    "    print(\"val tb:\",len(val_tb_files))\n",
    "    print(\"train normal:\",len(train_normal_files))\n",
    "    print(\"train covid:\",len(train_covid_files))\n",
    "    print(\"train pneumonia:\",len(train_pneumonia_files))\n",
    "    print(\"train tb:\",len(train_tb_files))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_aug_normal,train_aug_covid,train_aug_pneumonia,train_aug_tb=data_augmentation(train_normal_files,train_covid_files,train_pneumonia_files,train_tb_files)\n",
    "    test_aug_normal,test_aug_covid,test_aug_pneumonia,test_aug_tb=no_data_augmentation(test_normal_files,test_covid_files,test_pneumonia_files,test_tb_files)\n",
    "    val_aug_normal,val_aug_covid,val_aug_pneumonia,val_aug_tb=no_data_augmentation(val_normal_files,val_covid_files,val_pneumonia_files,val_tb_files)\n",
    "    \n",
    "    train_full_data,train_full_label=making_full_data(train_aug_normal,train_aug_covid,train_aug_pneumonia,train_aug_tb)  #getting my full data\n",
    "    test_full_data,test_full_label=making_full_data(test_aug_normal,test_aug_covid,test_aug_pneumonia,test_aug_tb)\n",
    "    val_full_data,val_full_label=making_full_data(val_aug_normal,val_aug_covid,val_aug_pneumonia,val_aug_tb)\n",
    "    \n",
    "    train_full_data,train_full_label= making_training_and_testing_data(train_full_data,train_full_label) #dividing full_data into train and test data\n",
    "    test_full_data,test_full_label=making_training_and_testing_data(test_full_data,test_full_label)\n",
    "    val_full_data,val_full_label=making_training_and_testing_data(val_full_data,val_full_label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939408d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as rp\n",
    "from classification_models.keras import Classifiers\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xp\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "def ensemble_gamma(X_train,X_test,X_val,y_train,y_test,y_val):\n",
    "    model1 = pickle.load(open('/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/SE_INCEPTION_V3_MODEL_4_class.sav', 'rb'))\n",
    "\n",
    "    model2 = pickle.load(open('/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/DenseNet201_model_4_class.sav', 'rb'))\n",
    "    model3 = pickle.load(open('/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/se_squeeznet_MODEL_4_class.sav', 'rb'))\n",
    "    testing_data=X_test\n",
    "    #testing_data=rp(testing_data)\n",
    "    test3 = model1.predict(testing_data)\n",
    "    model1.evaluate(testing_data,y_test)\n",
    "    testing_data=np.stack((X_test,)*3,axis=-1)\n",
    "    test4 = model2.predict(testing_data)\n",
    "    model2.evaluate(testing_data,y_test)\n",
    "    testing_data=X_test\n",
    "    #testing_data=xp(testing_data)\n",
    "    test5=model3.predict(testing_data)\n",
    "    model3.evaluate(testing_data,y_test)\n",
    "    \n",
    "    ans=[]\n",
    "    for i in tqdm(range(len(test5))):\n",
    "        \n",
    "        models_probs=[]\n",
    "        models_probs.append(list(test3[i]))\n",
    "        models_probs.append(list(test4[i]))\n",
    "        models_probs.append(list(test5[i]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        R_L = []\n",
    "        for k in range(len(models_probs)):\n",
    "            lis=[]\n",
    "            probs=models_probs[k]\n",
    "\n",
    "            for j in range(len(probs)):\n",
    "                if probs[j]==0:\n",
    "                    probs[j]=0.000000001\n",
    "                lis.append(math.gamma(probs[j]))\n",
    "                \n",
    "            R_L.append(lis)\n",
    "        R_L=np.array(R_L)\n",
    "\n",
    "        models_probs=np.array(models_probs)\n",
    "        RS=[]\n",
    "        \n",
    "        for row in range(len(R_L[0])):\n",
    "            RS.append(np.sum(R_L[:,row]))\n",
    "       \n",
    "        sum_for_CFS = []\n",
    "        for count in range(len(R_L[0])):\n",
    "            sum_for_CFS.append(np.sum(models_probs[:,count]))\n",
    "        \n",
    "        \n",
    "        for i in range(len(sum_for_CFS)):\n",
    "            sum_for_CFS[i] = ((4-sum_for_CFS[i])/len(models_probs))\n",
    "        cfs = sum_for_CFS\n",
    "       \n",
    "       \n",
    "        FDS=[]\n",
    "        for c in range(len(R_L[0])):\n",
    "            FDS.append(RS[c]*cfs[c])\n",
    "     \n",
    "        \n",
    "        prediction=np.argmin(FDS)\n",
    "        ans.append(prediction)\n",
    "        \n",
    "    test_labels=y_test\n",
    "    \n",
    "    \n",
    "    predictions = ans\n",
    "    \n",
    "   \n",
    "    y_label=np.argmax(y_test,axis=1).tolist()\n",
    "    corect=0\n",
    "    for ing in range(len(predictions)):\n",
    "        if predictions[ing]==y_label[ing]:\n",
    "            corect+=1\n",
    "    print(\"Accuracy of ensemble gamma based:\",(corect/len(y_label))*100)\n",
    "\n",
    "    my_pred=[]\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==0:\n",
    "            my_pred.append('COVID-19')\n",
    "        elif predictions[i]==1:\n",
    "            my_pred.append('NORMAL')\n",
    "        elif predictions[i]==2:\n",
    "            my_pred.append('PNEUMONIA')\n",
    "        elif predictions[i]==3:\n",
    "            my_pred.append('TB')\n",
    "    y_labels=[]\n",
    "    for i in range(len(y_label)):\n",
    "        if y_label[i]==0:\n",
    "            y_labels.append('COVID-19')\n",
    "        elif y_label[i]==1:\n",
    "            y_labels.append('NORMAL')\n",
    "        elif y_label[i]==2:\n",
    "            y_labels.append('PNEUMONIA')\n",
    "        elif y_label[i]==3:\n",
    "            y_labels.append('TB')\n",
    "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = ['COVID-19','NORMAL','PNEUMONIA','TB'], \n",
    "                     columns = ['COVID-19','NORMAL','PNEUMONIA','TB'])\n",
    "    \n",
    "#     plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.savefig(\"/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/Ensemble gamma 4 CLASS CLASSIFICATION  without segmentation.png\")\n",
    "    plt.show()\n",
    "    print(\"micro precision score of Ensemble gamma:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro precision score of Ensemble gamma:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro recall score of Ensemble gamma:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro recall score of Ensemble gamma:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro f1 score of Ensemble gamma:\",metrics.f1_score(predictions, y_label, average='micro'))\n",
    "    print(\"macro f1 score of Ensemble gamma:\",metrics.f1_score(predictions, y_label, average='macro'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_gamma(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "import os,glob\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from classification_models.keras import Classifiers\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "#import clahe\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from skimage.transform import radon, rescale\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from skimage import feature\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from tensorflow.keras.layers import Layer\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def se_incpetion_v3(X_train,X_test,X_val,y_train,y_test,y_val):\n",
    "    #pickled_model = pickle.load(open('SE_INCEPTION_V3.sav', 'rb'))\n",
    "    pickled_model = pickle.load(open('/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/SE_INCEPTION_V3_MODEL_4_class.sav', 'rb'))\n",
    "    testing_data=X_test\n",
    "    pred=pickled_model.predict(testing_data)\n",
    "    predictions = np.argmax(pred,axis = 1)\n",
    "    y_label=np.argmax(y_test,axis = 1)\n",
    "    print(\"Accuracy of SE Inception V3 is:\",accuracy_score(predictions,y_label))\n",
    "    my_pred=[]\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==0:\n",
    "            my_pred.append('COVID-19')\n",
    "        elif predictions[i]==1:\n",
    "            my_pred.append('NORMAL')\n",
    "        elif predictions[i]==2:\n",
    "            my_pred.append('PNEUMONIA')\n",
    "        elif predictions[i]==3:\n",
    "            my_pred.append('TB')\n",
    "    y_labels=[]\n",
    "    for i in range(len(y_label)):\n",
    "        if y_label[i]==0:\n",
    "            y_labels.append('COVID-19')\n",
    "        elif y_label[i]==1:\n",
    "            y_labels.append('NORMAL')\n",
    "        elif y_label[i]==2:\n",
    "            y_labels.append('PNEUMONIA')\n",
    "        elif y_label[i]==3:\n",
    "            y_labels.append('TB')\n",
    "    \n",
    "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = ['COVID-19','NORMAL','PNEUMONIA','TB'], \n",
    "                     columns = ['COVID-19','NORMAL','PNEUMONIA','TB'])\n",
    "    \n",
    "#     plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.savefig(\"/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/SE Inception V3, 4 CLASS CLASSIFICATION Confusion matrix without segmentation.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"micro precision score of SE Inception V3:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro precision score of SE Inception V3:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro recall score of SE Inception V3:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro recall score of SE Inception V3:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro f1 score of SE Inception V3:\",f1_score(predictions, y_label, average='micro'))\n",
    "    print(\"macro f1 score of SE Inception V3:\",f1_score(predictions, y_label, average='macro'))\n",
    "    print(\"roc auc score of SE Inception V3\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
    "    #covid=[1,0,0]\n",
    "    #normal=[0,1,0]\n",
    "    #pneumonia=[0,0,1]\n",
    "    \n",
    "\n",
    "    \n",
    "import keras\n",
    "from classification_models.keras import Classifiers\n",
    "def densenet(X_train,X_test,X_val,y_train,y_test,y_val):\n",
    "    #pickled_model = pickle.load(open('SE_RESNEXT_101.sav', 'rb'))\n",
    "    pickled_model = pickle.load(open('/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/DenseNet201_model_4_class.sav', 'rb'))\n",
    "    testing_data=np.stack((X_test,)*3,axis=-1)\n",
    "    \n",
    "    pred=pickled_model.predict(testing_data)\n",
    "    predictions = np.argmax(pred,axis = 1)\n",
    "    y_label=np.argmax(y_test,axis = 1)\n",
    "    print(\"Accuracy of DENSENET 201 is:\",accuracy_score(predictions,y_label))\n",
    "    my_pred=[]\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==0:\n",
    "            my_pred.append('COVID-19')\n",
    "        elif predictions[i]==1:\n",
    "            my_pred.append('NORMAL')\n",
    "        elif predictions[i]==2:\n",
    "            my_pred.append('PNEUMONIA')\n",
    "        elif predictions[i]==3:\n",
    "            my_pred.append('TB')\n",
    "    y_labels=[]\n",
    "    for i in range(len(y_label)):\n",
    "        if y_label[i]==0:\n",
    "            y_labels.append('COVID-19')\n",
    "        elif y_label[i]==1:\n",
    "            y_labels.append('NORMAL')\n",
    "        elif y_label[i]==2:\n",
    "            y_labels.append('PNEUMONIA')\n",
    "        elif y_label[i]==3:\n",
    "            y_labels.append('TB')\n",
    "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = ['COVID-19','NORMAL','PNEUMONIA','TB'], \n",
    "                     columns = ['COVID-19','NORMAL','PNEUMONIA','TB'])\n",
    "#     plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.savefig(\"/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/DENSENET 201 Confusion matrix, 4 CLASS CLASSIFICATION without segmentation.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"micro precision score of DENSENET 201:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro precision score of DENSENET 201:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro recall score of DENSENET 201:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro recall score of DENSENET 201:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro f1 score of DENSENET 201:\",f1_score(predictions, y_label, average='micro'))\n",
    "    print(\"macro f1 score of DENSENET 201:\",f1_score(predictions, y_label, average='macro'))\n",
    "    print(\"roc auc score of DENSENET 201:\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
    "    \n",
    "    \n",
    "\n",
    "def se_squeeznet(X_train,X_test,X_val,y_train,y_test,y_val):\n",
    "    #pickled_model = pickle.load(open('normal_densent_201.sav', 'rb'))\n",
    "       \n",
    "    pickled_model = pickle.load(open('/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/se_squeeznet_MODEL_4_class.sav', 'rb'))\n",
    "    \n",
    "    pred=pickled_model.predict(X_test)\n",
    "    predictions = np.argmax(pred,axis = 1)\n",
    "    y_label=np.argmax(y_test,axis = 1)\n",
    "    print(\"Accuracy of se_squeeznet is:\",accuracy_score(predictions,y_label))\n",
    "    my_pred=[]\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i]==0:\n",
    "            my_pred.append('COVID-19')\n",
    "        elif predictions[i]==1:\n",
    "            my_pred.append('NORMAL')\n",
    "        elif predictions[i]==2:\n",
    "            my_pred.append('PNEUMONIA')\n",
    "        elif predictions[i]==3:\n",
    "            my_pred.append('TB')\n",
    "    y_labels=[]\n",
    "    for i in range(len(y_label)):\n",
    "        if y_label[i]==0:\n",
    "            y_labels.append('COVID-19')\n",
    "        elif y_label[i]==1:\n",
    "            y_labels.append('NORMAL')\n",
    "        elif y_label[i]==2:\n",
    "            y_labels.append('PNEUMONIA')\n",
    "        elif y_label[i]==3:\n",
    "            y_labels.append('TB')\n",
    "    \n",
    "    cm=metrics.confusion_matrix(my_pred,y_labels)\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm,\n",
    "                     index = ['COVID-19','NORMAL','PNEUMONIA','TB'], \n",
    "                     columns = ['COVID-19','NORMAL','PNEUMONIA','TB'])\n",
    "    \n",
    "#     plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.savefig(\"/home/sriparna/s_sharma/Gama based/4_CLASS_CLASSIFICATION_WITH_TB_RESULTS/se_squeeznet Confusion matrix 4 CLASS CLASSIFICATION without segmentation.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"micro precision score of se_squeeznet:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro precision score of se_squeeznet:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro recall score of se_squeeznet:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro recall score of se_squeeznet:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro f1 score of se_squeeznet:\",f1_score(predictions, y_label, average='micro'))\n",
    "    print(\"macro f1 score of se_squeeznet:\",f1_score(predictions, y_label, average='macro'))\n",
    "    print(\"roc auc score of se_squeeznet:\",roc_auc_score(y_test, pred,multi_class='ovr'))\n",
    "    #covid=[1,0,0]\n",
    "    #normal=[0,1,0]\n",
    "    #pneumonia=[0,0,1]\n",
    "\n",
    "\n",
    "\n",
    "se_incpetion_v3(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of se inception v3 comes from this function\n",
    "densenet(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of se resnext 101 comes from this function\n",
    "se_squeeznet(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #results of densenet 201 comes from this function\n",
    "# ensemble_sugeno(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label) #ensemble results comes from this function\n",
    "#labels,predictions=rank_fuzzy(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label)\n",
    "# ensemble_choquest(train_full_data,test_full_data,val_full_data,train_full_label,test_full_label,val_full_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d54dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
